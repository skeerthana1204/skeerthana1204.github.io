[
  {
    "objectID": "posts/Blog_5/Anomaly detection.html",
    "href": "posts/Blog_5/Anomaly detection.html",
    "title": "Anomaly/outlier detection",
    "section": "",
    "text": "In machine learning, anomaly detection is a critical task that finds uncommon occurrences, deviations, or outliers in a dataset. These anomalies frequently reflect occurrences or trends that differ significantly from what is usual or from expected behavior. Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a distinct anomaly detection technique that uses the density distribution of data points instead of predefined thresholds to detect outliers.\n\nDBSCAN:\nOne well-known clustering algorithm for detecting outliers and clustering dense areas in data is DBSCAN. DBSCAN defines clusters as areas of high density divided by regions of low density, in contrast to typical clustering methods that assume a fixed number of clusters. With the help of this method, DBSCAN can effectively detect noise or outliers and handle clusters with irregular shapes. we can see below how DBSCAN Detects Anomalies:\nDensity-Based Approach: DBSCAN uses two parameters to characterize clusters: Epsilon, or eps,: Defines the range in which nearby points should be looked for. Minimum samples (min_samples): Specifies the minimum number of points needed to determine a core point inside the epsilon radius.\nClassification of Points: Core points are those locations inside a cluster that have a minimum of min_samples points neighborhood. Border Points: Those that are not core points , but are within an epsilon radius of a core point. Points that are neither core nor border points are known as noise or outlier points.\n\n\nCode:\nThis code shows how to use the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm for anomaly or outlier detection. It also shows how to cluster data using synthetic data sets created by make_moons and make_circles from the sklearn.datasets.\nImporting Libraries and Generating Moon-Shaped Data:\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\n\n#Generate Moons Datase\nX,y=make_moons(n_samples=250,noise=0.10)\n\nThe function make_moons() creates a toy dataset of points with the shape of two half circles that overlap, or moons. Noise adds random noise to the data points, and n_samples is the number of samples. The data point coordinates are contained in X, and the labels that correspond to them are included in Y.\nVisualizing Moon-Shaped Data:\n\nplt.scatter(X[:,0],X[:,1],c=y)\n\n&lt;matplotlib.collections.PathCollection at 0x20da20bfc10&gt;\n\n\n\n\n\nStandardize the Data:\nHere, it utilizes StandardScaler to standardize the data so that each column has a zero mean and a unit variance. fits a DBSCAN object to the scaled data by setting its epsilon (eps) to 0.5.\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# Apply DBSCAN on Moons Dataset\nX_scaled = scaler.fit_transform(X)\ndbscan=DBSCAN(eps=0.5)\ndbscan.fit(X_scaled)\ndbscan.labels_\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)\n\n\nVisualize DBSCAN Results on Moons Dataset:\nPlots the moons dataset once more, but this time, the points are colored according to the cluster labels that DBSCAN assigned.\n\nplt.scatter(X[:,0],X[:,1],c=dbscan.labels_)\n\n&lt;matplotlib.collections.PathCollection at 0x20da2090a50&gt;\n\n\n\n\n\nGenerate Circles Dataset:\ngenerates a dataset with 750 samples that show a factor of 0.3 and a noise level of 0.1, resulting in interlacing circles. Fits a second DBSCAN object to the new dataset by setting its epsilon to 0.1. Plots the dataset with circles, and colors the dots according to the cluster labels that the second DBSCAN iteration assigned.\n\nfrom sklearn.datasets import make_circles\n\nX, y = make_circles(n_samples=750, factor=0.3, noise=0.1)\n\n#Visualize the Circles Dataset:\nplt.scatter(X[:,0],X[:,1])\n\n# Apply DBSCAN on Circles Dataset\ndbcan=DBSCAN(eps=0.1)\ndbcan.fit_predict(X)\n\n#Visualize DBSCAN Results on Circles Dataset\nplt.scatter(X[:,0],X[:,1],c=dbcan.labels_)\n\n&lt;matplotlib.collections.PathCollection at 0x20da21216d0&gt;\n\n\n\n\n\n\n\nConclusion:\nWith an epsilon value of 0.5, DBSCAN identifies clusters quite effectively for the moons dataset, differentiating between the two moon forms according to their proximity. However, because the circles dataset is sensitive to the epsilon parameter, DBSCAN finds it difficult to cluster the data into meaningful groups when the epsilon value is 0.1. Instead of creating recognized clusters, it classifies the majority of the points as noise or outliers. In conclusion, modifying parameters is critical to DBSCAN’s performance, particularly the epsilon value (eps). When the clusters are well-separated and have a similar density, like in the moons dataset, it tends to perform better. The effectiveness of the algorithm to identify clusters and anomalies in different datasets can be greatly impacted by changing the parameters."
  },
  {
    "objectID": "posts/Blog_3/Linear and nonlinear regression.html",
    "href": "posts/Blog_3/Linear and nonlinear regression.html",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "Linear Regression:\nA fundamental statistical method for modeling the relationship between one or more independent variables (features) and a dependent variable (target) is called “linear regression”. The relationship between the predictor(s) and the target variable is assumed to be linear. For a single independent variable in simple linear regression, the model equation is as follows:\ny=mx+c\nIn order to minimize the difference between the predicted and actual values, linear regression finds the line that best fits the data. Its interpretability and simplicity make it widely used.\n\n\nNon-linear Regression using Polynomial Features:\nPolynomial regression is a type of non-linear regression that extends linear regression by considering polynomial relationships between the independent and dependent variables. It includes polynomial terms (quadratic, cubic, etc.) into the model equation to capture more intricate relationships than linear models can. An example of a quadratic polynomial regression model’s equation might be:\n\ny=c+m1​x+m2​x2\nThe model can capture non-linear patterns by changing the original features into polynomial features of a given degree using PolynomialFeatures, which can be found in libraries such as scikit-learn. In circumstances in which the relationship isn’t strictly linear, this flexibility allows the model to fit more complex relationships in the data, potentially increasing its predictive power over linear regression.\nDepending on the underlying relationship between the variables, either the polynomial or linear regression techniques are used, each with their advantages. While polynomial regression offers greater flexibility in capturing complex patterns, it can be more prone to overfitting with higher-degree polynomials. In contrast, linear regression is straightforward and easily interpreted.\n\n\n\nCode:\nThe code snippets below show the application of both linear and non-linear regression techniques to the dataset “column_2C_weka.csv” in order to predict the “sacral slope” based on the feature “pelvic_incidence.” Below is a brief overview of every section:\nData and Library Imports: NumPy, Pandas, and Matplotlib are among the necessary libraries that must be imported. opens a Pandas DataFrame and loads the dataset.\nExamining Data: Using data, shows the first few rows of the dataset.head (). uses data to provide information about the structure of the dataset.information(). employs data to present descriptive statistics.explain().\n\n\nLinear Regression:\nFilters the data for the ‘Abnormal’ class using a scatter plot. generates a scatter plot for the ‘Abnormal’ class between the variables ‘pelvic_incidence’ and’sacral_slope’.\nEvaluation and Prediction: Creates a Linear Regression model from scratch using scikit-learn. ‘pelvic_incidence’ as input (‘x’) and’sacral_slope’ as output (‘y’) help fit the model. calculates and outputs the linear regression model’s R2 score. Plots the scatter plot and regression line.\n\n# Linear Regression\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Load data\ndata = pd.read_csv('column_2C_weka.csv')\n\n# Explore data\ndata.head()\ndata.info()\ndata.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 310 entries, 0 to 309\nData columns (total 7 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   pelvic_incidence          310 non-null    float64\n 1   pelvic_tilt numeric       310 non-null    float64\n 2   lumbar_lordosis_angle     310 non-null    float64\n 3   sacral_slope              310 non-null    float64\n 4   pelvic_radius             310 non-null    float64\n 5   degree_spondylolisthesis  310 non-null    float64\n 6   class                     310 non-null    object \ndtypes: float64(6), object(1)\nmemory usage: 17.1+ KB\n\n\n\n\n\n\n\n\n\npelvic_incidence\npelvic_tilt numeric\nlumbar_lordosis_angle\nsacral_slope\npelvic_radius\ndegree_spondylolisthesis\n\n\n\n\ncount\n310.000000\n310.000000\n310.000000\n310.000000\n310.000000\n310.000000\n\n\nmean\n60.496653\n17.542822\n51.930930\n42.953831\n117.920655\n26.296694\n\n\nstd\n17.236520\n10.008330\n18.554064\n13.423102\n13.317377\n37.559027\n\n\nmin\n26.147921\n-6.554948\n14.000000\n13.366931\n70.082575\n-11.058179\n\n\n25%\n46.430294\n10.667069\n37.000000\n33.347122\n110.709196\n1.603727\n\n\n50%\n58.691038\n16.357689\n49.562398\n42.404912\n118.268178\n11.767934\n\n\n75%\n72.877696\n22.120395\n63.000000\n52.695888\n125.467674\n41.287352\n\n\nmax\n129.834041\n49.431864\n125.742385\n121.429566\n163.071041\n418.543082\n\n\n\n\n\n\n\n\n# Create a linear regression model\nreg = LinearRegression()\n\n# Filter data for 'Abnormal' class\ndata1 = data[data['class'] =='Abnormal']\n\n# Prepare features and target variable\nx = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\n\n# Visualize the data\nplt.figure(figsize=[10,10])\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()\n\n# Create a prediction space\npredict_space = np.linspace(min(x), max(x)).reshape(-1,1)\n\n# Fit the model\nreg.fit(x,y)\n\n# Predict\npredicted = reg.predict(predict_space)\n\n# R^2 \nprint('R^2 score: ',reg.score(x, y))\n\n# Plot regression line and scatter\nplt.plot(predict_space, predicted, color='black', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.title('linear Regression')\nplt.show()\n\n\n\n\nR^2 score:  0.6458410481075871\n\n\n\n\n\n\n\nNon Linear Regression:\nMakes a scatter plot for the ‘Abnormal’ class by connecting the points “pelvic_incidence” and “sacral_slope.”\nPolynomial Regression: Converts ‘x’ (a feature) into polynomial features of degree three by using PolynomialFeatures from scikit-learn. fits a linear regression model (x_poly) to the data after transformation.\nEvaluation and Prediction: generates a range of values for the prediction. makes use of the polynomial regression model to predict’sacral_slope’. calculates and outputs the polynomial regression model’s R2 score. Plots the scatter plot of the real data and the polynomial regression line.\n\n# Non-Linear Regression\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Load data\ndata = pd.read_csv('column_2C_weka.csv')\n\n# Filtering data for 'Abnormal' class\ndata1 = data[data['class'] =='Abnormal']\n\n# Extracting features and target variable\nx = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\n\n# Scatter plot of the data\nplt.figure(figsize=[10, 10])\nplt.scatter(x=x, y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.title('Non-linear Regression')\nplt.show()\n\n# Transforming feature 'x' to include polynomial features\npoly = PolynomialFeatures(degree=3)  # You can change the degree of the polynomial\nx_poly = poly.fit_transform(x)\n\n# Fitting polynomial regression model\nreg = LinearRegression()\nreg.fit(x_poly, y)\n\n# Generating a range of values for prediction\npredict_space = np.linspace(min(x), max(x), 100).reshape(-1, 1)\npredict_space_poly = poly.transform(predict_space)\n\n# Predictions using the polynomial regression model\npredicted = reg.predict(predict_space_poly)\n\n# R^2 score\nr2_score = reg.score(x_poly, y)\nprint('R^2 score: ', r2_score)\n\n# Plotting regression line and scatter plot\nplt.figure(figsize=[10, 10])\nplt.plot(predict_space, predicted, color='black', linewidth=3, label='Polynomial Regression')\nplt.scatter(x=x, y=y, label='Actual data')\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.title('Non-linear Regression')\nplt.legend()\nplt.show()\n\n\n\n\nR^2 score:  0.6739189064848099\n\n\n\n\n\n\n\nConclusion:\nUsing a linear relationship, this model predicts “sacral_slope” based solely on “pelvic_incidence.” How well the linear regression model fits the observed data is indicated by the R^2 score of 0.6458. While a perfect fit would be indicated by a R^2 score of 1.0, in this case, 0.6458 indicates that a linear model can account for roughly 64.58% of the variance in the’sacral_slope’.\nA polynomial regression using PolynomialFeatures is used to execute out a non-linear regression. ‘pelvic_incidence’ and ‘sacral_slope’ can have a more flexible relationship in this model because polynomial terms (such as cubic or quadratic terms) are included. This non-linear model’s R^2 score of 0.6739 suggests that it performs better than the linear model, indicating that the polynomial regression is able to capture the variation in ‘sacral_slope’ based on ‘pelvic_incidence’.\nThe non-linear model appears to fit the data slightly better, capturing more variability in “sacral_slope” using “pelvic_incidence” and the polynomial features produced from it, as evidenced by the improvement in R^2 score from 0.6458 (linear) to 0.6739 (non-linear). This might be because these variables have a non-linear relationship that the linear model is unable to adequately capture."
  },
  {
    "objectID": "posts/Blog_1/Probability theory and Random Variables.html",
    "href": "posts/Blog_1/Probability theory and Random Variables.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "In order to comprehend uncertainty and unpredictability, one of the foundational areas of mathematics is probability theory. In several domains, such as finance, machine learning, and statistics, it is essential. Primarily, probability theory addresses the possibility of several results in ambiguous circumstances.\n\nProbability Theory:\nIn order to quantify uncertainty and provide well-informed predictions, probability theory introduces concepts and tools. It involves an analysis of occurrences, their probabilities, and the laws that control them. In most cases, probability is represented by a number between 0 and 1, where 0 signifies impossible, 1 implies certainty, and values in between represent degrees of probable.\nWhen working with models that generate predictions based on noisy or incomplete data, it is imperative to understand probability. Probability theory is a fundamental component of machine learning algorithms that deal with uncertainty, like the Gaussian Naive Bayes model that we will be using in this section.\n\n\nRandom Variables:\nIn probability theory, random variables are a fundamental concept. They function as a link between the real-world phenomena we want to model and the abstract ideas of probability. A random variable is one whose potential values are determined by chance events. These numbers correspond to probabilities, which express how likely it is for each result.\nWorking with models such as Gaussian Naive Bayes in machine learning requires a solid understanding of random variables. The features of this model are assumed to have a Gaussian (normal) distribution, which is defined by its mean and variance. Models are able to produce probabilistic predictions because random variables help reflect the inherent unpredictability in data.\n\n\nGaussian Naive Bayes:\nIn Gaussian Naive Bayes, continuous attributes are taken into account and the data features are distributed Gaussianly over the dataset. According to the terminology used by the Sklearn library, Gaussian Naive Bayes is a kind of classification method that is based on the Naive Bayes algorithm and operates on continuous normally distributed information. Probability theory is utilized by the Gaussian Naive Bayes model to generate predictions for classification problems. Assuming that the features are conditionally independent given the class label, it is especially helpful when working with continuous data. For every class, the model calculates the probability of observing a given set of feature values using the Gaussian distribution.\n\n\nCode:\nThe Gaussian Naive Bayes model for stock prediction and the visualization of probability distributions for the predicted classes is shown in the example below.\n\nThe CSV file “stocks.csv,” which is a dataset from Kaggle, is used by the code.\n\nData Loading and Exploration: Initially, the code loads a dataset named “stocks.csv” using pandas and imports the required libraries. df.head() shows the dataset’s initial few rows, and df.isnull().sum() looks for any missing values.\nFeature Selection and Target Variable: The model has features like ‘Open, High, Low, Close, Adjust Close, and Volume’ chosen. ‘Ticker’, which is probably the stock ticker symbol, is the intended variable.\nData Splitting: Using train_test_split from scikit-learn, the dataset is split into training and testing sets (80/20 split).\nGaussian Naive Bayes Model: First, a model with the initialization parameter set to GaussianNB() is created.\nTraining the Model: model.fit(X_train, y_train) is used to train the model using the training data.\nProbabilities and Predictions: Model.predict(X_test) uses the training model to predict the classes on the test set. To extract the probabilities for each class, use model.predict_proba(X_test).\nVisualization of Probability Distributions: The code creates a histogram for each stock ticker in the dataset, which shows the expected probabilities for that ticker. Using matplotlib, it generates distinct probability distribution plots that illustrate the distribution of expected probabilities for every stock ticker in the test set.\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\n\n# Read the dataset named \"stocks.csv\"\n\ndf = pd.read_csv(\"stocks.csv\")\n\n# preview the dataset\ndf.head()\n\n# Checking for null values in the DataFrame and getting the sum of null values for each column\ndf.isnull().sum()\n\n\n# Selecting features and target variable\nfeatures = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\ntarget = 'Ticker'  # Assuming Ticker is the target variable\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n\n# Initializing Gaussian Naive Bayes model\nmodel = GaussianNB()\n\n# Fitting the model on the training data\nmodel.fit(X_train, y_train)\n\n# Making predictions on the test set\npredictions = model.predict(X_test)\n\n# Extracting probabilities for each class\nprobabilities = model.predict_proba(X_test)\n\n# Plotting histograms for the predicted probabilities of each class\nfor i, ticker in enumerate(model.classes_):\n    plt.figure(figsize=(8, 6))\n    plt.hist(probabilities[:, i], bins=30, alpha=0.7, label=ticker)\n    plt.title(f'Probability Distribution for Ticker: {ticker}')\n    plt.xlabel('Probability')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion:\nThe code ultimately generates a series of histograms, each of which shows the probability distribution for a specific stock ticker. Based on the test data, these visuals show how confident the model is in its predictions for each class (stock ticker). Understanding the degree of uncertainty surrounding the model’s predictions is made easier by the histograms."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Machine learning blog - Keerthana Sherikar"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MLBlogGithub",
    "section": "",
    "text": "Probability theory and random variables\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nKeerthana Sherikar\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nKeerthana Sherikar\n\n\n\n\n\n\n  \n\n\n\n\nLinear and nonlinear regression\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nKeerthana Sherikar\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nKeerthana Sherikar\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/outlier detection\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nKeerthana Sherikar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Blog_2/Classification.html",
    "href": "posts/Blog_2/Classification.html",
    "title": "Classification",
    "section": "",
    "text": "Classification is a supervised machine learning method where the model tries to predict the correct label of a given input data. In classification, the model is fully trained using the training data, and then it is evaluated on test data before being used to perform prediction on new unseen data.\n\nlogistic Regression:\nBinary classification issues can be solved using the approach of logistic regression. This method makes use of the logistic function, sometimes known as the sigmoid function, which is an S-shaped curve that can take any real number as input and map it to a value between 0 and 1, but never precisely inside those bounds. Consequently, logistic regression models the likelihood of the default class (i.e., the likelihood that an input (X) is a member of the default class (Y=1)) (P(X)=P(Y=1|X)). Log-odds or probit can be obtained by using the logistic function to forecast the probability. The model is therefore a linear combination of the inputs, but this linear combination is related to the default class’s log-odds.\nInitially, a model instance with default values was created. Indicate what the regularization strength in 10 is inverse to. After using the training data to train the logistic regression model, it was applied to the test data.\n\n\nCode:\nThe code performs to predict a binary outcome (‘Result’) by using logistic regression on a dataset that was loaded from ‘lung_cancer_examples.csv’.\nData Loading and Preprocessing: Pandas is used to load the ‘lung_cancer_examples.csv’ dataset from Kaggle which is a smaple dataset. removes the ‘Name’ and ‘Surname’ columns since they may not be necessary for classification. eliminates any rows that have missing values.\nData splitting divides the dataset into the target variable (Y) and characteristics (X), with the goal variable seeming to be “Result,” which indicates if lung cancer is present or absent. normalizes the feature values between 0 and 1 by applying Min-Max scaling to the features.\nTrain-Test Split: Using train test split from scikit-learn, divides the dataset into training and testing sets (90% training and 10% testing).\nLogistic Regression Model: Establishes a logistic regression model with a regularization value of ten (C=10). applies the logistic regression model fit to the X_train and Y_train training data.\nEvaluation and Predictions: Using the trained logistic regression model, predicts the target variable on the test set (X_test). determines the confusion matrix and shows it to assess how well the model performs on the test set. determines accuracy as the model’s test set performance metric. Plots the ROC curve and determines the AUC (Area Under the Curve) to show the true positive rate against the false positive rate, which represents the model’s performance.\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix, roc_curve\n\n# Load data\ndata = pd.read_csv('lung_cancer_examples.csv')\nprint('Dataset:', data.shape)\ndata.info()\n\n# Dropping columns\ndata1 = data.drop(columns=['Name', 'Surname'], axis=1)\ndata1 = data1.dropna(how='any')\nprint(data1.shape)\ndata1.head()\n\n# Splitting into features and target variable\nY = data1['Result']\nX = data1.drop(columns=['Result'])\n\n# Feature scaling\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Splitting the dataset into the Training set and Test set\nX_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.1, random_state=9)\n\n# Model initialization and training\nlogreg = LogisticRegression(C=10)\nlogreg.fit(X_train, Y_train)\n\n# Predictions\nY_predict1 = logreg.predict(X_test)\n\n# Confusion matrix\nlogreg_cm = confusion_matrix(Y_test, Y_predict1)\nplt.figure(figsize=(5, 5))\nsns.heatmap(logreg_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', cmap=\"YlGnBu\")\nplt.title('Logistic Regression Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# Test score\nscore_logreg = logreg.score(X_test, Y_test)\nprint(\"Accuracy:\", score_logreg)\n\n# ROC Curve\nY_predict1_proba = logreg.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict1_proba)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve - Logistic Regression')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nDataset: (59, 7)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 59 entries, 0 to 58\nData columns (total 7 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   Name     59 non-null     object\n 1   Surname  59 non-null     object\n 2   Age      59 non-null     int64 \n 3   Smokes   59 non-null     int64 \n 4   AreaQ    59 non-null     int64 \n 5   Alkhol   59 non-null     int64 \n 6   Result   59 non-null     int64 \ndtypes: int64(5), object(2)\nmemory usage: 3.4+ KB\n(59, 5)\nAccuracy: 1.0\n\n\n\n\n\n\n\n\n\n\nConclusion:\nIn conclusion, the code loads the data, cleans, and prepares the lung cancer dataset, trains a logistic regression model, evaluates its effectiveness using an accuracy score and confusion matrix, and shows the ROC curve. The outcome for the logistic regression model on the test set comprises the ROC curve, accuracy score, and confusion matrix."
  },
  {
    "objectID": "posts/Blog_4/Clustering.html",
    "href": "posts/Blog_4/Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is the task of dividing the population or data points into multiple groups so that the data points within those groups are more similar to each other than the data points outside of those categories. To put it another way, the goal is to group people who share similar characteristics together and place them in clusters.\n\nKMeans Clustering:\nThe unsupervised machine learning algorithm K-means clustering is among the most simple and popular. To determine how many centroids you require in the dataset, you will define a target number, k. The actual or hypothetical site that serves as the cluster’s centroid can be anywhere. Once the in-cluster sum of squares is reduced, each data point is allocated to one of the clusters. Stated differently, the K-means algorithm finds k centroids and, after the centroids are as small as possible, assigns each data point to the closest cluster. The K-means algorithm uses the centroid to calculate the average of the data, or “means.”\n\n\nCode:\nThe KMeans clustering implementation for a dataset of Mall_Customers is shown in the code below. Below is an explanation of the code:\nData loading: Imports of matplotlib, pandas, numpy, seaborn, and plotly libraries are made. A Pandas DataFrame (df) is loaded with the dataset (Mall_Customers.csv). Utilizing procedures like head(), info(), describe(), and isnull(), preliminary data studies is carried out.sum().\nData Visualization: To visualize the distributions of the columns “Age,” “Annual Income (k\\(),\" and \"Spending Score (1-100),\" histograms (distplots) are plotted. 'Spending Score (1-100)', 'Annual Income (k\\))‘, and ’Age’ are plotted against each other, with ‘Gender’ serving as a pointer. The relationship between ‘Age’ and ‘Spending Score’ is shown using scatter plots.\nKMeans Clustering: ‘Age’ and ‘Spending Score (1-100)’ (X1) columns are used to create the dataset on KMeans clustering. The inertia (within-cluster sum of squares) is then calculated by the algorithm through a loop for a range of cluster numbers (from 1 to 14). Plotting this will assist in determining the optimum number of clusters.\nApplying KMeans Clustering:  KMeans clustering is applied using k=4 and k=5 independently (n_clusters = 4 and n_clusters = 5). For both k=4 and k=5, scatter plots with cluster assignments (labels1) and centroids are made.\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.cluster import KMeans\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv('Mall_Customers.csv')\ndf.head()\n\ndf.columns\n\ndf.info()\n\ndf.describe()\n\ndf.isnull().sum()\n\nplt.figure(1 , figsize = (15 , 6))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1\n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n    sns.distplot(df[x] , bins = 15)\n    plt.title('Distplot of {}'.format(x))\nplt.show()\n\nsns.pairplot(df, vars = ['Spending Score (1-100)', 'Annual Income (k$)', 'Age'], hue = \"Gender\")\n\nplt.figure(1 , figsize = (15 , 7))\nplt.title('Scatter plot of Age v/s Spending Score', fontsize = 20)\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, s = 100)\nplt.show()\n\nX1 = df[['Age' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 15):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X1)\n    inertia.append(algorithm.inertia_)\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 15) , inertia , 'o')\nplt.plot(np.arange(1 , 15) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   CustomerID              200 non-null    int64 \n 1   Gender                  200 non-null    object\n 2   Age                     200 non-null    int64 \n 3   Annual Income (k$)      200 non-null    int64 \n 4   Spending Score (1-100)  200 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 7.9+ KB\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplying KMeans for k=4\n\nalgorithm = (KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n\n\n\n\n\n\nApplying KMeans for k=5\n\nalgorithm = (KMeans(n_clusters = 5, init='k-means++', n_init = 10, max_iter=300, \n                        tol=0.0001, random_state= 111 , algorithm='elkan'))\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n\n\n\n\n\n\nConclusion:\n‘Age’ and ‘Spending Score (1-100)’-based clusters are visualized by the code. For two distinct numbers of clusters (k=4 and k=5), scatter plots and overlays of cluster centroids are made in order to comprehend the distribution of consumers according to these features. The KMeans algorithm’s many clusters are shown by the color differentiation. Based on their age and mall buying patterns, these analyses helps in the understanding of possible client categorizations"
  }
]